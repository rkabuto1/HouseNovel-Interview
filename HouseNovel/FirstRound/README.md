# ğŸ—‚ï¸ HouseNovel Final Round Assignment: README

---

## Phase One: OCR + Structured Data Extraction (1900â€“1950)

Using scanned Minneapolis city directories from 1900 to 1950, extract and structure the following data for each resident entry:  
https://box2.nmtvault.com/Hennepin2/jsp/RcWebBrowse.jsp

â€¢ First and last name  
â€¢ Spouse name (if listed)  
â€¢ Home address  
â€¢ Residence indicator (e.g., h., res.)  
â€¢ Occupation  
â€¢ Employer/business name and address (if applicable)  
â€¢ Year of directory  

**Output format:** JSON  
Please include notes on any formatting challenges and how you addressed them.

---

## Phase Two: Real-World Application â€“ 1807 Dupont Ave S (built 1902)

Apply your workflow to the following address to create a timeline of past residents between 1902 and 1950:

**Address:**  
1807 Dupont Ave S, Minneapolis, MN 55403  

**Zillow listing:**  
https://www.zillow.com/homedetails/1807-Dupont-Ave-S-Minneapolis-MN-55403/1951320_zpid  

For each year the home is listed, include:  
â€¢ Resident full name(s)  
â€¢ Spouse name (if listed)  
â€¢ Occupation and employer  
â€¢ Business address (if listed)  
â€¢ Any formatting inconsistencies, gaps, or assumptions  
â€¢ Notes on how you validated or cross-referenced the accuracy  

This simulates how we use directory data in actual home history reports.

---

## Accuracy Expectations

We recognize OCR accuracy will vary by decade and layout. As a general benchmark:

- 1850sâ€“1870s: ~50â€“65% OCR-only accuracy; 90â€“100% with AI + manual QA  
- 1880sâ€“1899: ~65â€“80% OCR-only accuracy; 95â€“100% with AI + manual QA  
- 1900â€“1950: ~85â€“95% OCR-only accuracy; 98â€“100% with AI + manual QA  

---

## Additional Question

Many of our future datasets involve handwritten records from the 1800s. Do you have any experience with handwriting OCR or manual transcription, and would you feel comfortable working with handwritten materials?

---

## Phase 1  
### Thought Process Behind Each Step

I approached Phase 1 as a data extraction and quality assurance pipeline, where OCR text from historical directories needed to be parsed, structured, and validated with high accuracy. I broke the problem into three logical phases: parsing, cleaning, and output generation.

Throughout the process, I assumed that perfect OCR text could not be guaranteed, so I treated this as a fault tolerant system favoring clear logic, validation, and explainability over fragile overfitting. This structure also prepares the codebase for future enhancements like employer extraction, multi-line entry stitching, or handwriting model integration.

---

### OCR Text as Input (`ocr_output.txt`)

I assumed the OCR engine would produce line by line text closely resembling the original directory layout. To maximize parsing accuracy, I manually reviewed a few pages across different years to identify recurring patterns like name, occupation, residence marker (r, b, rms), and address.

---

### Parsing with `parse_residents.py`

I wrote a regular expression to capture the structural components of each entry. My assumption was that names start the line, followed by occupation and then a residence indicator and address. I included optional fields like middle name and spouse, anticipating variants like â€œ(wid John)â€ that carry semantic meaning. The output is structured JSON, which makes it easy to work with programmatically and scalable for downstream use.

---

### Cleaning with `clean_json.py`

I treated the initial parsed data as a raw layer that still required filtering and post processing. I applied logic to reclassify widow information into the spouse field and eliminate junk records based on name keywords like â€œHotelâ€ or â€œFlats.â€ I also filtered out incomplete addresses, which would degrade data quality in real applications. This separation of concerns ensures parsing and QA are independently testable.

---

### Final Outputs

I split valid data into `cleaned_residents_1912.json` and logged problematic rows in `rejected_entries.txt`. This was intentional as I wanted to preserve transparency, identify edge cases for future rule tuning, and show a pipeline mindset.

---

### Automation with Makefile

To streamline execution, I created a `Makefile` that runs everything in sequence.

---

### Explanation

The script processes OCR derived text from the 1912 Minneapolis city directory to extract structured resident data in JSON format. Each output entry includes first and last name, spouse name if listed, residence indicator (such as â€œrâ€ for resides, â€œbâ€ for boards, or â€œrmsâ€ for rooms), home address, occupation, and placeholders for employer and business address. The year of the directory is manually specified.  
The parsing logic assumes that entries begin with a last name followed by a first name and, optionally, a middle initial. If the line contains a pattern like â€œ(wid Geo W)â€, it is interpreted as widow status, and â€œGeo Wâ€ is assigned to the `spouse_name` field. The occupation is considered to be the text immediately following the name up to the residence indicator. Any entries with names that include terms such as â€œHotel,â€ â€œFlats,â€ or â€œBuildingâ€ are rejected, as these are more likely commercial listings or OCR errors. Entries with incomplete addressesâ€”such as those containing only a number without a street nameâ€”are also filtered out to preserve data integrity.  
Several formatting challenges were encountered. Many entries were wrapped or broken across lines due to OCR artifacts, making employer extraction and address parsing inconsistent. Additionally, abbreviations like "salon" (salesman) or â€œelkâ€ (clerk) were common and required contextual inference. Some addresses were partially mangled by OCR, such as â€œ4th a? Sâ€ instead of â€œ4th av S.â€  
For example, â€œRemsan Katharine (wid Geo W) b 3113â€ was originally parsed with the widow notation incorrectly assigned to the occupation field. This was corrected by post processing logic. Another error involved â€œHotel Berkeleyâ€ being parsed as a resident, which was later filtered using keyword-based rejection. Employer and business address extraction remain as future improvements.

---

## How to run my code

1. Ensure you have Python 3 installed.  
2. Place your OCR text file in the project directory as `ocr_output.txt`  
3. Run the full pipeline using:

This will:
    - Parse ocr_output.txt into structured_residents_1912.json
    - Clean and validate the data
    - Output the final result to cleaned_residents_1912.json
    - Log rejected entries to rejected_entries.txt

To remove all generated files, run:
make clean

1. ocr_output.txt â€“ Raw OCR text extracted from a scanned city directory page.
2. parse_residents.py â€“ Parses the OCR text and extracts resident data into structured JSON format.
3. structured_residents_1912.json â€“ Output file containing all parsed entries, including unfiltered data.
4. clean_json.py â€“ Filters and cleans the parsed data by removing invalid entries and correcting field assignments.
5. cleaned_residents_1912.json â€“ Final, cleaned JSON output ready for submission.
6. rejected_entries.txt â€“ Log of entries that were rejected during the cleaning step for QA review.
7. Makefile â€“ Automates the entire Phase 1 pipeline, from parsing to cleaning and output generation.

Phase 2 - https://www.zillow.com/homedetails/1807-Dupont-Ave-S-Minneapolis-MN-55403/1951320_zpid
1807 Dupont Ave S (built 1902)

Apply your workflow to the following address to create a timeline of past residents between 1902 and 1950:

In Phase 2, I focused on systematically constructing a reliable historical timeline of residents at 1807 Dupont Ave S using a combination of OCR processing, manual verification, and prompt-based information extraction. The following steps were completed:

### Targeted OCR Keyword Detection
I wrote a Python script that parsed OCR-scanned city directory text files and automatically searched for instances of the address â€œ1807 Dupontâ€. The script highlighted relevant lines containing the address, allowing me to quickly locate and isolate potential resident entries across years.

### Validation and Archive Collection (1902â€“1950)
After identifying hits from the OCR output, I manually confirmed the legitimacy of each entry by verifying that the page referenced the correct address and included resident-level information. I continued this process until I had coverage from 1902 through 1950, with each page manually validated and archived for accuracy.

### Handling Non-Listings and Missing Data
For years in which no valid references to 1807 Dupont Ave S existed, I confirmed this both through OCR and direct page inspection. I clearly marked these entries in the final JSON output using a standardized â€œDirectory not available or address not found in listingsâ€ message, ensuring transparency and completeness.

### Prompt-Based Structured Extraction (Prompt Engineering)
Using the validated OCR text from each relevant year, I crafted carefully constructed prompts and applied prompt engineering to extract structured data in JSON format, including fields such as full_name, spouse_name, occupation, employer, and home_address. This allowed me to maintain consistency across decades of historical records despite variation in formatting and terminology.

